{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask=np.load('test_mask.npy')\n",
    "train_mask=np.load('train_mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62454 62454\n"
     ]
    }
   ],
   "source": [
    "print(len(test_mask),len(train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "print(test_mask[0],train_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert simpletransformers text representation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62454 3\n",
      "62454 2783\n"
     ]
    }
   ],
   "source": [
    "path='../11.8/'\n",
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "topic=read_list(path+'topic.txt')\n",
    "print(len(topic),len(topic[0]))\n",
    "path='../11.8/'\n",
    "def read_list(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n',''))\n",
    "    return lsit\n",
    "data_list=read_list( path+'data_list_clear_stop.txt')\n",
    "print(len(data_list),len(data_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train=[]\n",
    "topic_test=[]\n",
    "data_list_train=[]\n",
    "data_list_test=[]\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        topic_test.append(topic[i])\n",
    "        data_list_test.append(data_list[i])\n",
    "    if train_mask[i]:\n",
    "        topic_train.append(topic[i])\n",
    "        data_list_train.append(data_list[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43823 6178\n",
      "43823 6178\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list_train),len(data_list_test))\n",
    "print(len(topic_train),len(topic_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fenci(data_list):\n",
    "    data_list_split=[i.split(' ') for i in data_list]\n",
    "    for i in range(len(data_list_split)):\n",
    "        while '' in data_list_split[i]:\n",
    "            data_list_split[i].remove('')\n",
    "    return data_list_split\n",
    "data_list_train=fenci(data_list_train)\n",
    "data_list_test=fenci(data_list_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 3), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 4), (26, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 3), (53, 1), (54, 1), (55, 2), (56, 1), (57, 1), (58, 6), (59, 4), (60, 8), (61, 3), (62, 1), (63, 1), (64, 9), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 3), (73, 1), (74, 4), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 2), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 4), (101, 1), (102, 3), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 2), (117, 1), (118, 1), (119, 1), (120, 2), (121, 1), (122, 7), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 3)]]\n",
      "[[('Bill', 1), ('But', 3), ('California', 1), ('County', 1), ('David', 1), ('Federal', 1), ('He', 1), ('If', 1), ('It', 1), ('Its', 2), ('Jan', 1), ('Management', 1), ('Maybe', 1), ('National', 1), ('Nevada', 1), ('New', 1), ('Oregon', 1), ('Over', 1), ('P', 1), ('Press', 1), ('Saturday', 1), ('Steven', 1), ('This', 1), ('Though', 1), ('Times', 1), ('US', 4), ('W', 1), ('We', 2), ('Williams', 3), ('acknowledged', 1), ('adding', 1), ('armed', 2), ('arrived', 1), ('attorney', 2), ('authority', 1), ('behavior', 1), ('bought', 1), ('business', 1), ('calls', 1), ('comment', 1), ('commit', 1), ('community', 2), ('conflict', 1), ('contributed', 1), ('controversy', 1), ('convicted', 2), ('country', 1), ('courts', 1), ('crimes', 1), ('criminal', 1), ('criticized', 1), ('decision', 2), ('declined', 3), ('dollars', 1), ('efforts', 1), ('email', 2), ('failed', 1), ('fair', 1), ('family', 6), ('father', 4), ('federal', 8), ('fires', 3), ('fter', 1), ('gained', 1), ('government', 9), ('governments', 1), ('growth', 1), ('handful', 1), ('happening', 1), ('harder', 1), ('hold', 1), ('involved', 1), ('judge', 3), ('lan', 1), ('land', 4), ('law', 2), ('lawyer', 1), ('lawyers', 1), ('leaders', 1), ('legal', 1), ('maintained', 1), ('miles', 1), ('militia', 2), ('minimum', 1), ('months', 1), ('ny', 1), ('office', 1), ('officials', 1), ('opinion', 1), ('opposition', 1), ('paid', 1), ('paying', 1), ('piece', 1), ('plan', 2), ('playing', 1), ('port', 1), ('porters', 1), ('powerful', 1), ('pressure', 1), ('price', 1), ('prison', 4), ('prominent', 1), ('property', 3), ('prosecutors', 1), ('protect', 1), ('protecting', 1), ('protest', 1), ('rally', 1), ('reduce', 1), ('referring', 1), ('relatives', 1), ('remote', 1), ('remove', 1), ('respond', 1), ('rights', 2), ('rightwing', 1), ('ruled', 2), ('rules', 1), ('sell', 1), ('sentence', 1), ('served', 2), ('setting', 1), ('son', 7), ('sought', 1), ('speak', 1), ('spread', 1), ('ssociated', 1), ('standard', 1), ('standoff', 1), ('terms', 1), ('terrorists', 2), ('to', 1), ('town', 1), ('trial', 1), ('welcomed', 1), ('word', 1), ('wrote', 3)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "id2word = corpora.Dictionary(data_list_train)     # Create Dictionary\n",
    "id2word.filter_extremes(no_below=3, no_above=0.5, keep_n=3000)\n",
    "\n",
    "\n",
    "id2word.save_as_text(path+\"dictionary\")                   # save dict\n",
    "texts = data_list_train                           # Create Corpus\n",
    "corpus = [id2word.doc2bow(text) for text in texts]   # Term Document Frequency\n",
    "print(corpus[:1])\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "\n",
    "#词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "Dictionary(3000 unique tokens: ['Bill', 'But', 'California', 'County', 'David']...)\n"
     ]
    }
   ],
   "source": [
    "print(len(id2word))\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    " \n",
    "#根据语料库和字典生成模型\n",
    "tfidf_model = models.TfidfModel(corpus=corpus, dictionary=id2word)\n",
    "tfidf_model.save('test_tfidf.model') #保存模型到本地\n",
    "tfidf_model = models.TfidfModel.load('test_tfidf.model') #载入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (8, 2), (10, 1), (22, 1), (25, 2), (42, 1), (64, 1), (138, 2), (146, 1), (147, 2), (148, 1), (154, 3), (156, 1), (170, 1), (171, 6), (173, 1), (190, 1), (196, 1), (214, 1), (223, 1), (236, 3), (245, 1), (258, 3), (265, 1), (268, 1), (276, 1), (283, 1), (289, 3), (296, 1), (300, 1), (305, 1), (311, 1), (314, 1), (317, 1), (320, 1), (347, 1), (377, 2), (388, 1), (389, 1), (406, 1), (458, 1), (472, 1), (475, 2), (499, 3), (506, 4), (512, 1), (523, 7), (534, 1), (552, 2), (583, 1), (591, 1), (602, 3), (606, 1), (645, 1), (649, 5), (656, 1), (704, 1), (732, 1), (781, 1), (822, 1), (833, 1), (847, 1), (858, 1), (895, 1), (901, 1), (908, 4), (1017, 4), (1059, 1), (1114, 1), (1121, 1), (1128, 1), (1145, 2), (1179, 1), (1200, 1), (1281, 1), (1299, 1), (1319, 1), (1325, 1), (1342, 4), (1357, 1), (1428, 1), (1435, 1), (1518, 1), (1637, 1), (1677, 1), (1733, 1), (1816, 1), (1827, 1), (1842, 1), (1849, 1), (1903, 1), (2120, 1), (2296, 1), (2339, 1), (2361, 1), (2431, 1), (2617, 1), (2800, 1)] [(1, 0.0141546559412148), (8, 0.02919280433883418), (10, 0.06698701253418414), (22, 0.022161973782909097), (25, 0.044199715851835346), (42, 0.055012915147601316), (64, 0.0245500170447351), (138, 0.06702607368298781), (146, 0.03589871232723197), (147, 0.07724272236096984), (148, 0.04124288726060009), (154, 0.10187973671213794), (156, 0.041960499599206814), (170, 0.05468582457180727), (171, 0.20043539579911634), (173, 0.04693928754293806), (190, 0.043103528193565735), (196, 0.03457861550238601), (214, 0.0587554106775548), (223, 0.030834739824094088), (236, 0.18421730893903554), (245, 0.038215545733339), (258, 0.1828697138537694), (265, 0.05313277305334773), (268, 0.07513208499272914), (276, 0.04214777052733133), (283, 0.050467594737933524), (289, 0.0952095672365627), (296, 0.04305621282784126), (300, 0.03169635857485617), (305, 0.045125020697481893), (311, 0.0715619703164079), (314, 0.04978847344567613), (317, 0.08383318589143787), (320, 0.0527386801290436), (347, 0.04028792154452958), (377, 0.1354102842030883), (388, 0.04767508123707083), (389, 0.07371056444695101), (406, 0.056171470863684084), (458, 0.03980360762520767), (472, 0.07221314339765307), (475, 0.07924746775183616), (499, 0.1643969647299377), (506, 0.216627556103128), (512, 0.0500327614243143), (523, 0.3854924619183207), (534, 0.042295071484420604), (552, 0.11384479917154028), (583, 0.07766300816677818), (591, 0.057320799821037896), (602, 0.14290623235817923), (606, 0.05602763386993316), (645, 0.0457641954480286), (649, 0.3189190036997602), (656, 0.056624846336377925), (704, 0.05381639647813818), (732, 0.0664330917751401), (781, 0.03274196460173123), (822, 0.0705217771948371), (833, 0.08383318589143787), (847, 0.05747430172834194), (858, 0.06856284144250631), (895, 0.05929890922309141), (901, 0.04267394581165172), (908, 0.29820508904411935), (1017, 0.25720900153956155), (1059, 0.05923680011600028), (1114, 0.06663741027434932), (1121, 0.047257561134152096), (1128, 0.057288633514165674), (1145, 0.0795069597481015), (1179, 0.053421946475147745), (1200, 0.06670169113214726), (1281, 0.06726322318340078), (1299, 0.059219090097895756), (1319, 0.058964045215878814), (1325, 0.052764285250059134), (1342, 0.2388477384872742), (1357, 0.04247787014369376), (1428, 0.052121401354856926), (1435, 0.032800775384732646), (1518, 0.08417018711314028), (1637, 0.0532112239467272), (1677, 0.07781972722620883), (1733, 0.07285120204811227), (1816, 0.06001359057677163), (1827, 0.0626170224623293), (1842, 0.07866918261817285), (1849, 0.04978295604713531), (1903, 0.039029160873772985), (2120, 0.07739717707948211), (2296, 0.0630091189562657), (2339, 0.06738277941565209), (2361, 0.08273966299653095), (2431, 0.08178503319112214), (2617, 0.07931269878260729), (2800, 0.08499011111495164)]\n"
     ]
    }
   ],
   "source": [
    "texts = data_list_test\n",
    "corpus_test = [id2word.doc2bow(text) for text in texts]\n",
    "corpus_tfidf = [tfidf_model[doc] for doc in corpus_test]\n",
    "print(corpus_test[0],corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features=[]\n",
    "for i in corpus_tfidf:\n",
    "    new_feature=[0 for k in range(3000)]\n",
    "    for j in i:\n",
    "        new_feature[j[0]]=j[1]\n",
    "#         print(j)#(66, 0.17233049258408828)\n",
    "#     print(new_feature)\n",
    "    tfidf_features.append(new_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6178 3000\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_features),len(tfidf_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topic_test),len(topic_test[0]))\n",
    "y_pred = KMeans(n_clusters=135, random_state=9).fit(tfidf_features)\n",
    "print(metrics.adjusted_rand_score(topic_test, y_pred.labels_))\n",
    "print(metrics.adjusted_mutual_info_score(topic_test, y_pred.labels_)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bj1",
   "language": "python",
   "name": "bj1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
