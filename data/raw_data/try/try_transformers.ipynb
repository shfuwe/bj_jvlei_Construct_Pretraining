{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda_num = str(5)\n",
    "device_s = \"cuda:\" + cuda_num\n",
    "device = torch.device(device_s if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(text_path):\n",
    "    lsit = []\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            lsit.append(line_list.replace('\\n', ''))\n",
    "    return lsit\n",
    "\n",
    "import ast\n",
    "def read_list2(text_path):\n",
    "    lsit=[]\n",
    "    with open('%s' % text_path, 'r', encoding=\"utf8\") as f:  # 打开一个文件只读模式\n",
    "        line = f.readlines()  # 读取文件中的每一行，放入line列表中\n",
    "        for line_list in line:\n",
    "            line = ast.literal_eval(line_list.replace('\\n', ''))\n",
    "            lsit.append(line)\n",
    "    return lsit\n",
    "\n",
    "\n",
    "def store_list(lsit, text_path):\n",
    "    ff = open(text_path, encoding='utf-8', mode='w')\n",
    "    for line_list in lsit:\n",
    "        ff.write(str(line_list))  # 写入一个新文件中\n",
    "        ff.write(\"\\n\")\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    # 加载停用词\n",
    "    stop_words_path = \"../../stopwords0.txt\"\n",
    "    stopwords = set([item.strip() for item in open(stop_words_path, 'r').readlines()])\n",
    "    return stopwords\n",
    "    # print(stopwords)\n",
    "\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(stopwords, line):\n",
    "    line_clean = []\n",
    "    for word in line:\n",
    "        if word =='':\n",
    "            continue\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        line_clean.append(word)\n",
    "    return line_clean\n",
    "\n",
    "def get_st():\n",
    "    return read_list(\"../../st1.txt\")\n",
    "    \n",
    "    \n",
    "    \n",
    "def clear(line):\n",
    "    line_clean = []\n",
    "    for word in line:\n",
    "        if not word.isdigit():\n",
    "            line_clean.append(word)\n",
    "    return line_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20807\r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "st = get_st()\n",
    "data_list0 = read_list('./text.txt')\n",
    "# data_list0=data_list0[0:1]\n",
    "data_list=[]\n",
    "i_n=0\n",
    "for i in data_list0:\n",
    "    sys.stdout.write(str(i_n) + '\\r')\n",
    "    i_n=i_n+1\n",
    "    for j in st:#去除标点符号\n",
    "        i=i.replace(j,'')\n",
    "    data_list.append(i)\n",
    "# print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example,return_tensors=\"pt\",padding=True,truncation=True)\n",
    "\n",
    "tokenized_datasets = [tokenize_function(i) for i in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20808\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20807\r"
     ]
    }
   ],
   "source": [
    "modeled_datasets=[]\n",
    "i_n=0\n",
    "\n",
    "for i in range(len(tokenized_datasets)):\n",
    "    sys.stdout.write(str(i_n) + '\\r')\n",
    "    i_n=i_n+1\n",
    "    try0=tokenized_datasets[i].to(device)\n",
    "    out0 = model(**try0)\n",
    "    # print(outputs[1][0].tolist())\n",
    "    # print(len(outputs[1][0].tolist()))\n",
    "    modeled_datasets.append(out0[1][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list(modeled_datasets, 'modeled_datasets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20808\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(modeled_datasets))\n",
    "print(len(modeled_datasets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_datasets0=read_list2('modeled_datasets.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20808\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(modeled_datasets0))\n",
    "print(len(modeled_datasets0[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 512] at entry 0 and [1, 472] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34195/1846505627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_fields'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# namedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 512] at entry 0 and [1, 472] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_dataloader:\n",
    "    batch=batch.to(device)\n",
    "    outputs = model(**batch)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "squad_convert_examples_to_features() missing 4 required positional arguments: 'max_seq_length', 'doc_stride', 'max_query_length', and 'is_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7951/390690550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msquad_convert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m features = squad_convert_examples_to_features(\n\u001b[0;32m----> 3\u001b[0;31m     examples=data_list,  tokenizer=tokenizer)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: squad_convert_examples_to_features() missing 4 required positional arguments: 'max_seq_length', 'doc_stride', 'max_query_length', and 'is_training'"
     ]
    }
   ],
   "source": [
    "# from transformers import squad_convert_examples_to_features\n",
    "# features = squad_convert_examples_to_features(\n",
    "#     examples=data_list,  tokenizer=tokenizer,max_seq_length=512,doc_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7951/1373116649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36mglue_convert_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, task, label_list, output_mode)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_glue_convert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     return _glue_convert_examples_to_features(\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/fuwen/anaconda3/envs/bj1/lib/python3.7/site-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36m_glue_convert_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, task, label_list, output_mode)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using output mode {output_mode} for task {task}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlabel_from_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "# data_features = convert_examples_to_features(data_list,tokenizer,label_list=None,output_mode=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20808\n",
      "3\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets))\n",
    "print(len(tokenized_datasets[0]))\n",
    "print(len(tokenized_datasets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "tensor([[-0.6944,  0.2582, -0.0607,  ..., -0.4951,  0.5002,  0.3865],\n",
      "        [-0.2278,  0.0936, -0.6565,  ...,  0.5138,  1.1057, -0.1172],\n",
      "        [-0.5605,  0.2595,  0.5565,  ..., -0.2362, -0.6021,  0.1308],\n",
      "        ...,\n",
      "        [-0.8794, -0.6774, -0.0228,  ...,  0.1449, -0.1038,  0.5325],\n",
      "        [-0.1562, -0.3481, -0.1050,  ..., -0.0992,  0.4419, -0.1524],\n",
      "        [ 0.3078,  0.3086,  0.1337,  ..., -0.2235, -0.4089,  0.2020]],\n",
      "       device='cuda:5', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(outp[0][0]))\n",
    "print(outp[0][0])#该文章每个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "tensor([-6.9438e-01,  2.5816e-01, -6.0673e-02, -3.1512e-01, -3.2401e-01,\n",
      "        -1.3490e-02,  3.5672e-01,  6.8995e-01, -2.7142e-01,  1.7811e-01,\n",
      "        -1.3229e-01, -6.1978e-01, -9.6737e-01,  1.2803e-01,  3.0904e-02,\n",
      "         8.3929e-02, -2.9550e-01,  3.1537e-01,  2.8993e-01,  3.0545e-01,\n",
      "         5.3507e-02, -5.4684e-01,  2.3491e-01,  1.5954e-01,  6.7469e-01,\n",
      "        -6.0988e-01,  3.5565e-02, -6.9016e-01, -5.2587e-01, -2.8850e-01,\n",
      "         5.5732e-01,  1.2682e-01, -2.5702e-02,  3.8728e-02,  5.2750e-01,\n",
      "        -1.1681e-01,  8.0179e-01, -8.1869e-02,  4.4787e-01,  2.1115e-01,\n",
      "        -3.1938e-01,  6.7282e-01,  1.1077e-01,  7.6984e-02,  9.7705e-02,\n",
      "        -9.2495e-02, -3.2289e+00, -2.0712e-01, -2.0023e-01, -1.7386e-01,\n",
      "        -1.3848e-01, -1.1753e-02,  5.3485e-01, -4.5319e-02,  6.6432e-01,\n",
      "         6.3904e-01, -8.5435e-01, -2.2076e-01,  2.3065e-01, -1.7449e-01,\n",
      "         1.9379e-01,  7.6357e-02, -2.2030e-01,  1.7953e-02,  2.2015e-01,\n",
      "        -1.7139e-02, -2.3261e-01,  6.0209e-01, -9.3533e-01,  3.7253e-01,\n",
      "        -6.3789e-01,  3.4254e-02,  3.5268e-01, -3.2068e-02,  4.4373e-02,\n",
      "        -5.6412e-02, -4.3339e-01,  5.5190e-01, -6.0914e-02, -3.9648e-01,\n",
      "        -5.5902e-02,  2.9856e-01, -3.2395e-01,  4.1883e-01,  9.3507e-02,\n",
      "         4.4780e-01, -1.6716e-01,  2.1765e-01, -2.5698e-01,  6.3454e-01,\n",
      "         6.4996e-02,  2.9194e-01, -1.6007e-01, -3.6116e-02,  3.6284e-01,\n",
      "        -3.2021e-01,  3.1074e-01,  7.2764e-03,  1.5734e-01,  5.5124e-01,\n",
      "         3.6494e-01,  1.2017e-02,  5.3593e-01, -7.9733e-01,  6.1733e-01,\n",
      "        -2.0225e-01,  4.5888e-02, -6.3591e-01,  1.5518e-01, -2.2387e+00,\n",
      "         2.0134e-01,  8.0631e-01, -4.4630e-01, -3.6939e-01, -1.1584e-01,\n",
      "         3.5190e-01,  2.7817e-01, -1.0243e+00, -1.7649e-01,  1.8417e-01,\n",
      "        -1.9181e-01,  6.2518e-01, -6.3479e-01, -3.3517e-01, -1.3527e-01,\n",
      "         2.0758e-01,  3.6346e-01, -7.0855e-01,  3.8503e-01,  3.9749e-01,\n",
      "         4.1564e-01, -1.8854e-01, -5.0243e-01, -2.6816e-01, -7.8975e-01,\n",
      "         5.4469e-02,  6.8012e-01, -6.7017e-02,  3.0607e-01,  3.5652e-01,\n",
      "        -4.7069e-01, -6.9156e-01, -2.8619e+00, -4.1280e-02,  6.2077e-01,\n",
      "        -3.6855e-01, -1.5757e-01,  3.8720e-01, -4.0373e-01,  2.0957e-01,\n",
      "        -4.2339e-02, -5.5990e-01,  5.0685e-01, -7.5934e-02,  2.5294e-01,\n",
      "         5.0632e-01, -5.6462e-02, -7.1818e-01, -7.0988e-02,  3.6747e-01,\n",
      "         6.5023e-01,  3.0689e-01, -9.3116e-02,  1.9104e-01,  7.5475e-01,\n",
      "         2.0959e-01,  4.7540e-01,  1.5072e-04,  4.8541e-01,  3.0026e-01,\n",
      "        -2.2708e-02, -5.3038e-02,  1.5590e+00, -2.7643e-01,  3.4995e-01,\n",
      "         1.5222e-01, -3.8394e-01,  1.5293e-01,  3.8769e-01,  1.2218e-01,\n",
      "        -5.9730e-01, -1.9782e-01, -2.6922e-01,  5.4407e-01,  3.7440e-01,\n",
      "        -3.7385e-01,  4.4459e-01,  3.3513e-03,  6.2621e-02,  1.4506e-01,\n",
      "        -3.0515e-01, -5.6247e-01,  4.9154e-01,  3.6908e-01,  7.6989e-01,\n",
      "         1.5693e-01,  6.7108e-01,  5.5748e-02,  1.8125e-01,  5.8324e-01,\n",
      "        -3.0215e-01, -5.6203e-02,  3.3841e-01,  3.4119e-01, -5.9562e-01,\n",
      "         3.4627e+00,  4.5627e-01, -3.0761e-01, -3.9238e-01,  7.6476e-01,\n",
      "         2.2596e-02, -1.6711e-01, -3.8426e-01, -3.9539e-01,  4.6378e-01,\n",
      "         5.2664e-01, -2.0961e-01, -3.1500e-01, -2.1740e-01,  2.9777e-01,\n",
      "        -2.8946e-02,  1.1914e+00, -4.2959e-02, -1.3448e-01,  1.7485e-02,\n",
      "        -6.0449e-02, -1.1666e-01, -2.3293e-02,  1.9972e-02, -1.5868e+00,\n",
      "         2.8222e-01, -7.6851e-01, -2.1570e-01,  3.1493e-01, -2.2279e-01,\n",
      "        -5.0359e-01,  1.6279e-01,  8.8678e-02, -1.9571e-01,  2.9115e-02,\n",
      "         4.2495e-01,  8.4524e-01, -2.9009e-01,  8.3110e-02, -8.7556e-01,\n",
      "         1.8688e-01,  1.8722e-01,  3.3898e-01,  4.9067e-01,  6.1199e-01,\n",
      "         4.8582e-01,  5.3214e-02, -3.3473e-01, -6.4271e-02,  1.9216e-01,\n",
      "         2.9068e-01, -2.0280e-01,  1.8227e-01, -7.3979e-02,  4.1843e-02,\n",
      "        -4.0028e-01,  3.9689e-01,  4.8525e-01,  2.6224e-01, -1.4730e-01,\n",
      "        -1.3789e-01, -4.4034e-01, -4.4290e-01, -3.9073e-01, -5.1949e-01,\n",
      "         5.4242e-01, -4.0471e-01, -9.3106e-02, -2.8736e+00, -3.4451e-02,\n",
      "         1.6192e-01,  3.5049e-01,  1.5150e-01,  1.5555e-01,  2.4386e-01,\n",
      "        -7.8110e-02,  6.2604e-01, -3.7747e-01,  4.0058e-01,  4.1851e-01,\n",
      "        -2.5381e-01,  9.3176e-02, -3.8119e-01,  6.3672e-01,  6.9822e-01,\n",
      "         1.8910e-01, -2.0423e-01,  3.3164e-01,  2.8983e-01, -3.0535e-01,\n",
      "        -4.0619e-01,  4.4693e-02,  1.8427e-01, -1.3560e-01, -2.7272e-01,\n",
      "        -7.9553e-01,  6.9353e-01,  8.1507e-02,  1.5729e-01, -3.0496e-02,\n",
      "        -1.4294e-01, -4.4142e-01, -3.6960e-01, -2.7989e+00,  9.0115e-01,\n",
      "        -1.1127e-01, -9.4477e-01, -2.8033e-01, -1.3329e-01,  9.4966e-01,\n",
      "        -5.9343e-01, -6.3036e-01, -2.7862e-01,  4.0048e-01, -1.5118e-01,\n",
      "        -3.4643e-01,  5.6071e-01,  3.8784e-01,  9.6985e-01,  4.4410e-01,\n",
      "        -4.9550e-01,  7.0732e-01,  1.9445e-01, -1.2015e-01,  3.4482e-02,\n",
      "        -1.6033e-01, -1.2971e-01,  4.0229e-01,  1.9227e-01, -5.1084e-01,\n",
      "        -5.0005e-01, -1.6516e-01,  1.8838e-01,  3.8524e-01,  7.6913e-02,\n",
      "        -4.7258e-01, -3.6096e-01, -1.2858e-01,  3.1176e-02,  2.3727e-01,\n",
      "         1.6710e-01, -2.6762e-02, -5.4396e-01, -2.0993e-01,  8.3857e-01,\n",
      "         3.2717e-01,  2.1331e-01,  1.2052e+00, -8.2592e-02,  3.1751e-01,\n",
      "        -9.6350e-02, -6.3418e-01,  2.7475e-01, -1.6843e-01,  3.2356e-01,\n",
      "         5.5880e-01, -2.2990e-01, -3.5947e-01,  1.2047e-01,  5.0928e-01,\n",
      "        -1.5037e-01, -4.4038e-02, -2.6863e-01,  2.8108e-01, -1.0596e-01,\n",
      "         3.3560e-01,  7.8238e-02,  4.5781e-01, -5.9246e-01,  3.8767e-01,\n",
      "        -2.4935e-01, -1.5331e-02, -5.1201e-02, -4.4072e-01,  3.5886e-01,\n",
      "        -4.6233e-01, -1.0657e+00, -1.0653e-01, -6.1726e-01,  1.7652e-01,\n",
      "        -6.0511e-02,  7.5201e-01, -5.6316e-02, -2.9073e-01, -1.3698e-01,\n",
      "        -4.0618e-01,  4.0680e-01, -2.5725e-01,  1.2796e-01, -2.3293e-01,\n",
      "         3.9611e-01, -1.1137e+00, -6.4730e-01, -5.1215e-01, -6.9480e-02,\n",
      "        -1.8013e-01,  1.8095e-01,  1.2606e-01,  4.4556e-02,  3.8290e-01,\n",
      "        -9.7612e-01,  5.3728e-01, -1.9188e-01,  2.6400e-01,  2.7928e-01,\n",
      "         1.3007e-01, -4.1202e-01, -2.4287e-01, -8.5665e-02, -3.9057e-01,\n",
      "         8.5703e-01, -5.5035e-01, -4.4754e-02, -2.1067e-01, -1.0772e-01,\n",
      "        -9.1811e-02,  2.4707e-01,  9.0719e-01, -2.4585e-01, -3.1158e-01,\n",
      "         8.0389e-01,  4.8648e-01,  5.1033e-01,  4.2337e-01, -4.2337e-01,\n",
      "        -3.5705e-01,  3.4873e-02, -9.4236e-01, -5.9202e-01, -2.7216e-01,\n",
      "        -5.3190e-01, -2.5989e-01,  3.7237e-01,  8.3969e-02,  2.9643e-01,\n",
      "        -1.1093e+00, -6.7659e-01,  1.5569e-01, -4.1167e-01, -3.4815e-01,\n",
      "         5.3527e-01,  3.9372e-01, -2.8555e-01, -1.4183e-01,  1.5243e-01,\n",
      "        -5.3211e-01,  4.2875e-01,  1.2988e-01,  8.2569e-01,  5.2514e-01,\n",
      "         3.3161e-01, -3.3915e-01,  2.2906e-01, -3.6547e-01, -2.5148e-01,\n",
      "         6.3136e-02, -1.1788e+00,  5.6999e-01,  6.4874e-02, -1.8544e-01,\n",
      "         4.0523e-01, -7.9771e-01, -8.7869e-01,  2.8951e-02,  6.2354e-01,\n",
      "        -1.7819e+00, -2.8293e-02,  8.4063e-01,  4.8596e-01,  2.7349e-02,\n",
      "        -5.3825e-02, -1.5416e-01,  6.6803e-01, -2.2408e-01, -4.6185e-01,\n",
      "        -1.8358e-01, -3.9618e-01,  5.0139e-01, -5.2013e-01,  4.6195e-01,\n",
      "         4.6799e-01, -4.6508e-01, -4.2865e-01, -7.6218e-01,  6.5660e-01,\n",
      "        -8.1508e-01,  8.2672e-01, -4.5388e-01,  2.8123e-01,  2.5596e-01,\n",
      "        -3.4904e-01, -2.5268e-02,  1.9872e-01,  4.0256e-01,  7.2981e-01,\n",
      "        -4.7742e-01, -5.2972e-01, -9.6022e-01,  2.0714e-01,  4.3065e-01,\n",
      "         1.2678e-01,  6.9020e-02, -5.9933e-01,  9.7183e-01,  3.1629e-01,\n",
      "        -6.6275e-01,  4.1506e-01, -3.7758e-02,  7.3714e-01,  7.0440e-01,\n",
      "        -1.2123e-01, -3.9475e-01,  3.4757e-01,  6.5496e-02,  2.1741e-01,\n",
      "         5.5904e-01,  8.8191e-02, -5.1964e-01, -4.0083e-01, -3.6658e-02,\n",
      "        -2.2239e-01, -4.8109e-01,  3.9123e-01, -6.0344e-01, -1.3156e-01,\n",
      "        -6.0522e-02, -2.8014e-01, -2.9771e-01,  1.3566e-01, -3.2487e-01,\n",
      "        -9.2330e-01,  6.4311e-01, -5.4431e-01, -6.6517e-02,  4.3991e-01,\n",
      "         7.8527e-01,  4.0790e-01, -6.9129e-01,  6.0370e-01,  3.9872e-01,\n",
      "        -2.7050e-02,  4.6901e-02, -1.5999e-01,  3.5563e-01, -2.5669e-01,\n",
      "         3.5442e-01, -8.4591e-01, -6.6169e-01,  7.8105e-01,  8.0039e-03,\n",
      "         7.2716e-01, -5.0996e-01, -2.5075e-01, -1.5459e-01,  1.3597e-01,\n",
      "        -3.4889e-01, -8.3306e-01, -3.0690e-01,  1.9388e-01,  9.1035e-01,\n",
      "        -9.2894e-02,  2.4972e-01, -2.1015e-01,  4.9829e-01, -1.3629e-01,\n",
      "         7.5794e-02,  5.4845e-01,  5.3221e-01,  2.2585e-01,  7.4909e-01,\n",
      "         1.3646e-01,  4.6402e-01, -3.9597e-02, -5.1460e-01,  2.4587e-01,\n",
      "         1.4750e-01,  5.1525e-01, -3.0006e-01, -1.7128e-01,  1.3132e-01,\n",
      "        -4.8634e-01,  7.7561e-02, -3.2240e-01,  1.6557e+00,  1.6285e-01,\n",
      "         5.7751e-01, -1.8611e-01,  2.8992e-01, -2.9097e-01,  4.7201e-02,\n",
      "         2.0623e-01, -1.3687e-01,  3.2886e-03,  1.1817e-01,  1.7898e-01,\n",
      "        -3.1729e-01,  2.7242e-01,  6.3417e-01,  4.0607e-01, -1.3371e-01,\n",
      "        -5.3637e-01, -6.3365e-01, -4.4788e-01, -7.2589e-01,  1.1111e-01,\n",
      "         7.9711e-01,  4.3301e-01, -6.0975e-01,  4.8491e-01, -1.8854e-01,\n",
      "        -5.8421e-01,  7.5799e-01,  2.7421e-01, -6.2124e-01, -9.8997e-02,\n",
      "         5.3273e-01,  7.1969e-01, -5.4313e-01, -9.1102e-02,  2.5370e-01,\n",
      "        -1.1589e-01, -7.1684e-02, -1.5959e-01,  4.8807e-02,  1.0106e-01,\n",
      "         3.4868e-02, -1.3869e-01,  2.8777e-01,  4.7236e-01, -4.0132e-02,\n",
      "        -7.3903e-02,  1.1864e-01,  8.2046e-02, -3.2440e-01,  4.4236e-01,\n",
      "        -3.7100e-01, -5.4507e-02, -1.3704e-01, -5.3186e-01, -6.6231e-01,\n",
      "        -6.7460e-01,  2.4146e-02,  2.7091e-01, -1.4951e-01, -1.4347e-01,\n",
      "         1.4826e-01, -1.6222e-02, -7.0042e-01,  2.9496e-01, -7.1771e-01,\n",
      "         7.3779e-01, -7.4126e-02, -1.0598e-01,  5.3133e-01,  4.4412e-01,\n",
      "         1.2293e-01, -4.8228e-03, -5.2753e-02,  8.9073e-01,  4.7747e-01,\n",
      "         2.5423e-01, -3.0988e-01, -2.2839e+00,  6.1811e-02,  6.1131e-01,\n",
      "         2.4296e-01, -1.0418e-01,  2.5618e-01,  4.0403e-01,  1.1569e-02,\n",
      "        -2.6276e-01, -3.5509e-02,  2.6874e-01,  8.5977e-01, -8.4207e-02,\n",
      "        -1.3630e-01,  9.2904e-02,  9.3232e-02,  1.7974e-01, -6.3349e-01,\n",
      "        -1.0408e+00, -3.8559e-02,  2.5945e-01,  5.4279e-01, -5.5263e-01,\n",
      "        -6.7052e-01, -5.8985e-01, -1.3197e-01,  1.0097e+00, -6.5098e-01,\n",
      "         3.5663e-02,  2.5856e-01, -1.2283e-01,  6.1438e-01, -3.8024e-01,\n",
      "         2.8787e-01,  2.3554e-01, -1.0666e+00, -7.1517e-01,  2.0317e-01,\n",
      "        -2.6399e-01, -1.2510e-01, -1.1095e+00,  9.9930e-01, -2.0136e-01,\n",
      "        -2.0637e-01,  2.3001e-01,  1.3182e-01,  5.4043e-01, -6.0559e-01,\n",
      "         7.0064e-01, -1.8527e-01, -7.8701e-02,  8.0565e-02, -2.0909e-01,\n",
      "        -7.9903e-02,  4.3933e-01, -4.2302e-02,  7.0648e-01,  1.9632e-01,\n",
      "        -4.7688e-01, -4.6366e-01,  8.0771e-03, -4.6608e-01, -2.5869e-01,\n",
      "        -4.2305e-01,  4.6287e-01, -2.8624e-01, -1.9738e-01,  2.4575e-01,\n",
      "         3.4499e-01,  2.2048e-03,  2.0117e-02, -2.6337e-01,  2.4863e-01,\n",
      "         1.6659e-01,  1.7609e-01, -1.7905e-01,  6.7870e-01,  2.6773e-01,\n",
      "         1.1093e+00, -3.1148e-01, -3.9566e-01,  4.8692e-02, -4.3536e-02,\n",
      "         3.3933e-01,  2.1477e-01, -5.4984e+00, -4.2184e-01, -1.3020e-02,\n",
      "        -3.3861e-02,  2.3941e-01, -3.9322e-01,  9.1062e-02, -3.3661e-01,\n",
      "        -2.7521e-01, -9.7327e-02,  2.3104e-01, -3.8838e-01, -9.3811e-03,\n",
      "        -4.9506e-01,  5.0019e-01,  3.8652e-01], device='cuda:5',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(outp[0][0][0]))#某个词\n",
    "print(outp[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "tensor([-0.7637, -0.5992, -0.9091,  0.6618,  0.6772, -0.3101,  0.2112,  0.5965,\n",
      "        -0.7950, -1.0000, -0.6371,  0.9519,  0.9778,  0.3979,  0.7420, -0.6960,\n",
      "        -0.1789, -0.6718,  0.5241,  0.6210,  0.6425,  1.0000, -0.3876,  0.6070,\n",
      "         0.5447,  0.9773, -0.7986,  0.8754,  0.8854,  0.6707, -0.4393,  0.6491,\n",
      "        -0.9861, -0.4332, -0.9233, -0.9926,  0.6707, -0.4966, -0.1304, -0.3725,\n",
      "        -0.5985,  0.5542,  1.0000, -0.2291,  0.6128, -0.5382, -1.0000,  0.4731,\n",
      "        -0.6534,  0.8636,  0.7010,  0.9590,  0.4876,  0.5122,  0.6915, -0.7283,\n",
      "         0.0044,  0.2450, -0.5036, -0.7297, -0.6876,  0.5927, -0.8735, -0.7612,\n",
      "         0.8628,  0.8201, -0.6175, -0.5027, -0.3123,  0.1124,  0.6817,  0.5490,\n",
      "        -0.6130, -0.6185,  0.7043,  0.4654, -0.7679,  1.0000, -0.2923, -0.9639,\n",
      "         0.9553,  0.8460,  0.7228, -0.4939,  0.3926, -1.0000,  0.5922, -0.3953,\n",
      "        -0.9769,  0.5823,  0.6463, -0.4163,  0.8649,  0.7490, -0.3615, -0.6988,\n",
      "        -0.4211, -0.8904, -0.5597, -0.3072,  0.4614, -0.5091, -0.5831, -0.5764,\n",
      "         0.5057, -0.7105,  0.2794,  0.6751,  0.2931,  0.6854,  0.7045, -0.6069,\n",
      "         0.5458, -0.8712,  0.7467, -0.5089, -0.9747, -0.7416, -0.9861,  0.6266,\n",
      "        -0.5056, -0.3956,  0.7300, -0.5024,  0.6740, -0.4235, -0.9411, -1.0000,\n",
      "        -0.4392, -0.6360, -0.6296, -0.4925, -0.9607, -0.9364,  0.7556,  0.7934,\n",
      "         0.3794,  0.9999, -0.6183,  0.8850, -0.2836, -0.8526,  0.5483, -0.6681,\n",
      "         0.7057, -0.2536, -0.3403,  0.3987, -0.5729,  0.3693, -0.7404, -0.6023,\n",
      "        -0.7652, -0.5943, -0.6169,  0.8347, -0.5599, -0.9574, -0.1053, -0.5592,\n",
      "        -0.4935,  0.6768,  0.5940,  0.4933, -0.3968,  0.5952, -0.1426,  0.7088,\n",
      "        -0.6174, -0.3667,  0.5771, -0.6046, -0.9174, -0.9664, -0.5441,  0.4003,\n",
      "         0.9542,  0.7292,  0.5926,  0.8212, -0.5647,  0.7575, -0.9482,  0.9719,\n",
      "        -0.3545,  0.5491, -0.8941,  0.6887, -0.4695, -0.1007,  0.7740, -0.6283,\n",
      "        -0.6009, -0.4082, -0.6892, -0.6654, -0.9071,  0.3724, -0.4224, -0.5895,\n",
      "        -0.4009,  0.8352,  0.8849,  0.2091,  0.4473,  0.5277, -0.8110, -0.4334,\n",
      "         0.4203,  0.3983,  0.3047,  0.9724, -0.8887, -0.2840, -0.3780, -0.9695,\n",
      "         0.3750, -0.6016, -0.4663, -0.7445,  0.8256, -0.6488,  0.2768,  0.4735,\n",
      "        -0.0015, -0.6028,  0.4350, -0.6522,  0.5946, -0.4449,  0.9443,  0.9147,\n",
      "        -0.7555, -0.4009,  0.9537, -0.9525, -0.5505,  0.2170, -0.4592,  0.6453,\n",
      "        -0.8044,  0.9913,  0.8394,  0.4222, -0.8366, -0.9032, -0.2998, -0.6071,\n",
      "        -0.3309,  0.0575,  0.8598,  0.7792,  0.6534,  0.4758, -0.5845,  0.7782,\n",
      "        -0.9539, -0.9189, -0.9448, -0.5367, -0.9818,  0.8569,  0.4869,  0.6928,\n",
      "        -0.6958, -0.6726, -0.9033,  0.3388,  0.3453,  0.8297, -0.6639, -0.6540,\n",
      "        -0.6772, -0.8988,  0.2796, -0.3258, -0.4686,  0.3464, -0.7276,  0.6307,\n",
      "         0.5707,  0.6367, -0.9485,  0.9787,  1.0000,  0.9448,  0.6614,  0.3939,\n",
      "        -1.0000, -0.8805,  1.0000, -0.9965, -1.0000, -0.7487, -0.5896,  0.3907,\n",
      "        -1.0000, -0.4871, -0.2518, -0.7000,  0.7531,  0.9587,  0.7000, -1.0000,\n",
      "         0.8305,  0.8111, -0.7649,  0.9328, -0.5806,  0.9660,  0.5557,  0.6630,\n",
      "        -0.5049,  0.6756, -0.9277, -0.7513, -0.7532, -0.8871,  0.9995,  0.4682,\n",
      "        -0.7289, -0.7386,  0.6271, -0.2093,  0.0127, -0.9185, -0.5406,  0.6605,\n",
      "         0.6051,  0.1903,  0.4976, -0.4459,  0.5017,  0.2712, -0.1168,  0.7734,\n",
      "        -0.9401, -0.0654,  0.4743,  0.2203, -0.7615, -0.9754,  0.9116, -0.5805,\n",
      "         0.8727,  1.0000,  0.8568, -0.5351,  0.6751,  0.4688, -0.7181,  1.0000,\n",
      "         0.7512, -0.9605, -0.7941,  0.8603, -0.7802, -0.8057,  0.9997, -0.4551,\n",
      "        -0.7490, -0.3222,  0.9855, -0.9831,  0.9991, -0.6380, -0.8985,  0.8751,\n",
      "         0.8251, -0.5619, -0.5194,  0.3690, -0.5970,  0.6326, -0.3037,  0.7028,\n",
      "         0.6335, -0.1449,  0.6189,  0.1995, -0.7426,  0.4965, -0.7891, -0.5063,\n",
      "         0.9669,  0.6433, -0.3634,  0.4263, -0.5486, -0.8701, -0.9369,  0.5245,\n",
      "         1.0000, -0.6675,  0.7465, -0.4634, -0.3105,  0.2466,  0.7214,  0.7438,\n",
      "        -0.5090, -0.6928,  0.7381, -0.5418, -0.9918,  0.2197,  0.4614, -0.3828,\n",
      "         1.0000,  0.5321,  0.5213,  0.4887,  0.9878,  0.4223,  0.1888,  0.9226,\n",
      "         0.9552, -0.4745,  0.7952,  0.0927, -0.9126, -0.5256, -0.7430,  0.3463,\n",
      "        -0.8920, -0.2718, -0.8571,  0.8709,  0.9532,  0.5560,  0.5004,  0.7523,\n",
      "         1.0000, -0.9890,  0.4352,  0.7575,  0.0042, -0.9999, -0.4307, -0.6118,\n",
      "        -0.3559, -0.8436, -0.4004,  0.3984, -0.9354,  0.8656,  0.7438, -0.4909,\n",
      "        -0.9733, -0.6039,  0.5041,  0.4391, -0.9889, -0.5930, -0.6338,  0.5999,\n",
      "        -0.4972, -0.7258, -0.3470, -0.5750,  0.5198, -0.4779,  0.7641,  0.9045,\n",
      "         0.7769, -0.9086, -0.6344, -0.4023, -0.7416,  0.4366, -0.6774, -0.9326,\n",
      "        -0.5635,  1.0000, -0.6330,  0.8318,  0.3010, -0.0416, -0.5009,  0.5399,\n",
      "         0.9711,  0.5857, -0.7315, -0.8783,  0.6628, -0.6719,  0.6251,  0.7962,\n",
      "         0.7050,  0.5886,  0.9090,  0.5784, -0.3635,  0.3836,  0.9521, -0.5119,\n",
      "        -0.4302, -0.4474, -0.4004, -0.6425,  0.4973,  1.0000,  0.4818,  0.7110,\n",
      "        -0.9886, -0.8785, -0.8295,  1.0000,  0.7638, -0.4588,  0.7444,  0.3972,\n",
      "        -0.5114,  0.1476, -0.5174, -0.4382,  0.4617,  0.3721,  0.9533, -0.6265,\n",
      "        -0.9531, -0.6772,  0.5776, -0.8810,  1.0000, -0.7711, -0.5936, -0.2437,\n",
      "        -0.6288, -0.9062,  0.2030, -0.9498, -0.5624,  0.4214,  0.8990,  0.4174,\n",
      "        -0.8001, -0.7759,  0.8060,  0.7183, -0.8783, -0.8190,  0.9283, -0.8648,\n",
      "         0.5875,  1.0000,  0.6601,  0.1853,  0.4608, -0.1795,  0.5414, -0.3810,\n",
      "         0.4334, -0.7826, -0.4340, -0.4135,  0.5604, -0.4299, -0.9375,  0.4632,\n",
      "         0.4703, -0.7685, -0.7770, -0.5388,  0.6445,  0.8534, -0.6337, -0.3914,\n",
      "         0.2711, -0.3879, -0.8685, -0.6076, -0.5953, -1.0000,  0.5600, -1.0000,\n",
      "         0.7135,  0.2279, -0.4748,  0.6758,  0.7202,  0.8514, -0.3504, -0.8782,\n",
      "         0.2694,  0.6832, -0.4721, -0.2245, -0.3004,  0.5996, -0.3718,  0.4219,\n",
      "        -0.6792,  0.7845, -0.4356,  1.0000,  0.4860, -0.5713, -0.4043,  0.6687,\n",
      "        -0.5488,  1.0000, -0.1386, -0.9313,  0.5567, -0.7574, -0.5623,  0.5049,\n",
      "         0.3844, -0.7872, -0.9680,  0.6508,  0.5581, -0.7871,  0.6786, -0.5670,\n",
      "        -0.5039,  0.3869,  0.9045,  0.9675,  0.5550,  0.5656, -0.9360, -0.6706,\n",
      "         0.9068,  0.5675, -0.2122,  0.4150,  1.0000,  0.5851, -0.7312, -0.1953,\n",
      "        -0.6879, -0.4927, -0.5852,  0.5466,  0.4950,  0.7914, -0.4838,  0.9045,\n",
      "        -0.8902,  0.2740, -0.3451, -0.5882,  0.5864, -0.8663, -0.9753, -0.9742,\n",
      "         0.7190, -0.5665, -0.3846,  0.5879,  0.4686,  0.6226,  0.6664, -1.0000,\n",
      "         0.8966,  0.4926,  0.8609,  0.8929,  0.8489,  0.7232,  0.6059, -0.9600,\n",
      "        -0.2328, -0.6382, -0.3863,  0.5712,  0.6942,  0.6348,  0.5737, -0.5368,\n",
      "        -0.7509, -0.6711, -0.9337, -0.9774,  0.5964, -0.7474, -0.4558,  0.9030,\n",
      "         0.0320, -0.3658, -0.0660, -0.9503,  0.2219,  0.5532,  0.0899,  0.3769,\n",
      "         0.4393,  0.7187,  0.8190,  0.9845, -0.8875,  0.4793, -0.8278,  0.4948,\n",
      "         0.9588, -0.9230,  0.4966,  0.7878, -0.6140,  0.5634, -0.5554, -0.4597,\n",
      "         0.9129, -0.5214,  0.6664, -0.6331, -0.3290, -0.6508, -0.5331, -0.5267,\n",
      "        -0.7188,  0.7342,  0.1543,  0.7740,  0.8623, -0.4190, -0.5522, -0.4940,\n",
      "        -0.7481, -0.8174,  0.3986, -0.2451, -0.2264,  0.4690,  0.0977,  0.9917,\n",
      "         0.2376, -0.6446, -0.5560, -0.6964,  0.6695, -0.8280, -0.7427, -0.6565,\n",
      "         0.6678,  0.5440,  1.0000, -0.7691, -0.8858, -0.6384, -0.6094,  0.6338,\n",
      "        -0.4710, -1.0000,  0.3967, -0.7849,  0.7594, -0.4792,  0.8185, -0.7714,\n",
      "        -0.7570, -0.5742,  0.6625,  0.7046, -0.6208, -0.4653,  0.7160, -0.3548,\n",
      "         0.9733,  0.5735, -0.7467, -0.0109,  0.7791, -0.7134, -0.6513,  0.4748],\n",
      "       device='cuda:5', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(outp[1][0]))\n",
    "print(outp[1][0])#该文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bj1",
   "language": "python",
   "name": "bj1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
